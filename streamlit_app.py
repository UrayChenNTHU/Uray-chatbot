from openai import OpenAI
import time
import re
from dotenv import load_dotenv
import os
import subprocess
import sys
import udn_data_processing
from sklearn.metrics.pairwise import cosine_similarity
import json
from google import genai
from google.genai import types
import jieba
import ast
import pandas as pd
import logging
import io
from PIL import Image

# Import ConversableAgent class
import autogen
from autogen import ConversableAgent, LLMConfig
from autogen import AssistantAgent, UserProxyAgent, LLMConfig
from autogen.code_utils import content_str
from coding.constant import JOB_DEFINITION, RESPONSE_FORMAT

import streamlit as st


@st.cache_data(show_spinner=False)
def load_data():
    return udn_data_processing.load_and_tfidf()


# Load environment variables from .env file
load_dotenv(override=True)

# https://ai.google.dev/gemini-api/docs/pricing
# URL configurations
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
OPEN_API_KEY = st.secrets["OPEN_API_KEY"]
DEEPSEEK_API_KEY = st.secrets["Deepseek_KEY"]

placeholderstr = "Please input your command"
user_name = "User"
user_image = "https://www.w3schools.com/howto/img_avatar.png"

seed = 42

'''
llm_config_tokenizer = LLMConfig(
    api_type="openai",
    model="gpt-4o-mini",
    api_key=OPEN_API_KEY,
)

with llm_config_tokenizer:
    tokenizer_agent = ConversableAgent(
        name="tokenizer_agent",
        system_message=(
            "ä½ æ˜¯ä¸€å€‹ä¸­æ–‡æ–·è©å°ˆå®¶ã€‚\n"
            "è¼¸å…¥ï¼šä¸€æ®µä¸­æ–‡æ–‡å­—ã€‚è«‹å°å®ƒåšæ–·è©ï¼Œå°‡æ‰€æœ‰è©èªä»¥ Python list çš„å½¢å¼å›å‚³ã€‚\n"
            "ä¾‹å¦‚ï¼šè¼¸å…¥ã€Œæˆ‘æƒ³ç©å‹•ä½œéŠæˆ²ã€ï¼Œæ‡‰å›å‚³ ['æˆ‘', 'æƒ³', 'ç©', 'å‹•ä½œ', 'éŠæˆ²']ã€‚\n"
            "è«‹å‹™å¿…åªè¼¸å‡ºä¸€å€‹åˆæ³•çš„ Python listï¼Œä¸è¦é¡å¤–åŠ ä»»ä½•è§£é‡‹æ–‡å­—ã€‚"
        )
    )

def tokenize_by_llm(text: str) -> list:
    """
    æŠŠ text äº¤çµ¦ LLMï¼Œè¦æ±‚å®ƒç›´æ¥å›å‚³ Python list å½¢å¼çš„ token listã€‚
    è‹¥å¤±æ•—å°±å›å‚³ç©ºåˆ—è¡¨æˆ–åŸå§‹æ–‡å­—åš fallbackã€‚
    """
    prompt = f"è«‹å°ä»¥ä¸‹æ–‡å­—åšä¸­æ–‡æ–·è©ï¼Œä¸¦ä»¥ Python list å›å‚³ï¼š\n\n{text}"
    try:
        result = tokenizer_agent.run(task=prompt)
        return ast.literal_eval(result.final_output.strip())
    except Exception as e:
        logging.error(f"LLM æ–·è©å¤±æ•—ï¼š{e}")
        return []

def recommend_games(prompt, df, vectorizer, tfidf_matrix, top_n=5):
    # 1. ç”¨ LLM åšæ–·è©
    tokens = tokenize_by_llm(prompt)
    # 2. å¦‚æœ LLM å›å‚³ç©º listï¼Œfallback æŠŠæ•´æ®µç•¶ä¸€å€‹ token
    if not isinstance(tokens, list) or len(tokens) == 0:
        tokens = [prompt]
    # 3. ç”¨ç©ºæ ¼ä¸²èµ· tokensï¼Œå†åšå‘é‡åŒ–
    joined = " ".join(tokens)
    vec = vectorizer.transform([joined])
    sims = cosine_similarity(vec, tfidf_matrix).flatten()
    idxs = sims.argsort()[::-1][:top_n]
    recs = df.iloc[idxs].copy()
    recs["score"] = sims[idxs]
    return recs
'''

def recommend_games(prompt: str, df: pd.DataFrame, vectorizer, tfidf_matrix, top_n: int = 5):
    """
    ç¸½å…¥å£ï¼šå…ˆè«‹ LLM æ‹†è§£ã€Œå–œæ­¡ã€è·Ÿã€Œä¸å–œæ­¡ã€ï¼Œ
    å†æŠŠæ‹†å‡ºä¾†çš„ç‰‡æ®µå„è‡ªåš TF-IDFï¼Œæœ€å¾Œåˆä½µå‘é‡ç®—ç›¸ä¼¼åº¦ã€‚
    """
    # 1. ç”¨ LLM æ‹†æˆ like/dislike
    prefs = split_preferences_by_llm(prompt)
    print(prefs)
    positive_text = prefs.get("like", "").strip()
    negative_text = prefs.get("dislike", "").strip()

    # 2. å®šç¾©æŠŠæ–‡å­—è½‰æˆå‘é‡çš„ helper
    def text_to_vec(text: str):
        tokens = jieba.lcut(text)
        joined = " ".join(tokens)
        return vectorizer.transform([joined])

    # 3. å¦‚æœ positive ä¹Ÿç©ºï¼Œå°±ç›´æ¥ fallback æˆåŸæœ¬èˆŠç‰ˆï¼ˆå…¨éƒ¨ç•¶æˆä¸€æ®µæ­£é¢ï¼‰
    if not positive_text:
        positive_text = prompt

    pos_vec = text_to_vec(positive_text)
    neg_vec = None
    if negative_text:
        neg_vec = text_to_vec(negative_text)

    # 4. åˆä½µå‘é‡ï¼špos - neg
    if neg_vec is not None:
        combined_vec = pos_vec - neg_vec
    else:
        combined_vec = pos_vec

    # 5. ç”¨åˆæˆå‘é‡è·Ÿæ•´å€‹ tfidf_matrix ç®— cosine similarity
    sims = cosine_similarity(combined_vec, tfidf_matrix).flatten()

    # 6. æŒ‘ top_n
    idxs = sims.argsort()[::-1][:top_n]
    recs = df.iloc[idxs].copy()
    recs["score"] = sims[idxs]

    return recs

llm_config_gemini = LLMConfig(
    api_type = "google", 
    model="gemini-2.0-flash-lite",                    # The specific model
    api_key=GEMINI_API_KEY,   # Authentication
)

llm_config_openai = LLMConfig(
    api_type = "openai", 
    model="gpt-4o-mini",                    # The specific model
    api_key=OPEN_API_KEY,   # Authentication
)

with llm_config_gemini:
    assistant = AssistantAgent(
        name="assistant",
        system_message=(
        "You are a helpful assistant. "
        "Answer user questions appropriately. "
        "After your result, say 'ALL DONE'. "
        "Do not say 'ALL DONE' in the same response."
        ),
        max_consecutive_auto_reply=2
    )

with llm_config_openai:
    tokens_refiner = ConversableAgent(
        name="tokens_refiner",
        system_message=(
            "You are a Chinese token refinement expert.\n"
            "Input: a list of tokens, already stop-word filtered.\n"
            "Remove control characters, punctuation, meaningless tokens, and merge game-specific terminologies.\n"
            "Return the cleaned tokens as a Python list."
        )
    )


# 3. Helper to refine tokens via LLM
def refine_by_llm(token_list):
    prompt = f"Refine the following token list: {token_list}"
    try:
        
        result = tokens_refiner.run(task=prompt)
        
        return ast.literal_eval(result.final_output)
    except Exception as e:
        logging.error(f"Error refining tokens: {e}")
        return token_list

def refine_tokens_list(token_list):
    if not isinstance(token_list, list):
        return token_list
    
    return refine_by_llm(token_list)

gemini_client = genai.Client(api_key=GEMINI_API_KEY)
def split_preferences_by_llm(user_input: str) -> dict:
    """
    ç”¨ Google Gemini æŠŠä¸€å¥å«ã€Œå–œæ­¡ã€å’Œã€Œä¸å–œæ­¡ã€çš„ä¸­æ–‡æ‹†æˆ JSONï¼š
      { "like": "...", "dislike": "..." }
    å¦‚æœ Gemini å›å‚³é JSONï¼Œå°± fallback åªçµ¦ likeï¼ŒæŠŠ dislike è¨­ç©ºã€‚
    """
    system_instruction = (
        "ä½ æ˜¯ä¸€å€‹ä¸­æ–‡èªæ„åˆ†æå°ˆå®¶ï¼Œ"
        "èƒ½æŠŠä¸€æ®µåŒ…å«ã€Œå–œæ­¡ã€è·Ÿã€Œä¸å–œæ­¡ã€çš„å¥å­æ‹†æˆå…©å€‹æ¬„ä½ï¼š"
        "'like' è·Ÿ 'dislike'ï¼Œä»¥ JSON å›å‚³ã€‚"
        'ä¾‹å¦‚åŸå¥ç‚ºã€Œæˆ‘å–œæ­¡å¯æ¨‚ï¼Œä½†æ˜¯ä¸å–œæ­¡ç‰›å¥¶ã€å‰‡JSONç‚ºï¼š{ "like": "å¯æ¨‚", "dislike": "ç‰›å¥¶" }' \
        'ã€Œæˆ‘ä¸å–œæ­¡ä¸Šå­¸ï¼Œä¹Ÿä¸å–œæ­¡ç©è€ã€å‰‡JSONç‚ºï¼š{ "like": "...", "dislike": "ä¸Šå­¸ã€ç©è€" }' \
        'ã€Œæˆ‘å–œæ­¡çœ‹æµ·ï¼Œä¹Ÿå–œæ­¡çˆ¬å±±ã€å‰‡JSONç‚ºï¼š{ "like": "çœ‹æµ·ã€çˆ¬å±±", "dislike": "..." }'
        "è«‹åƒ…å›å‚³ç´” JSONï¼Œåƒè¬ä¸è¦å¤šåŠ ä»»ä½•è§£é‡‹æ–‡å­—ã€‚\n"
        "å¦‚æœå¥å­è£¡æ²’æœ‰ã€Œä¸å–œæ­¡ã€çš„éƒ¨åˆ†ï¼Œå°±æŠŠ dislike è¨­ç‚ºç©ºå­—ä¸²ã€‚"
        "ä¸­æ–‡èªªæ˜è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚"
    )
    user_message = f"è«‹æŠŠé€™æ®µæ‹†æˆ JSONï¼š\"{user_input}\""

    # 2. å‘¼å« Gemini
    resp = gemini_client.models.generate_content(
        model="gemini-2.0-flash-lite",  # æˆ–ä½ æœ‰æ¬Šé™çš„å…¶ä»– Gemini å‹è™Ÿ
        contents=user_message,
        config=types.GenerateContentConfig(
            system_instruction=system_instruction,
            temperature=0.0,
            max_output_tokens=200
        )
    )

    text = resp.text.strip()
    try:
        # å˜—è©¦ parse å›å‚³çµæœè£¡çš„ JSON
        obj = json.loads(text)
        like = obj.get("like", "").strip()
        dislike = obj.get("dislike", "").strip()
        return {"like": like, "dislike": dislike}
    except Exception:
        # parse å¤±æ•—å°± fallback
        return {"like": user_input, "dislike": ""}


user_proxy = UserProxyAgent(
    "user_proxy",
    human_input_mode="NEVER",
    code_execution_config=False,
    is_termination_msg=lambda x: content_str(x.get("content")).find("ALL DONE") >= 0,
)

# Function Declaration 

def stream_data(stream_str):
    for word in stream_str.split(" "):
        yield word + " "
        time.sleep(0.05)

def save_lang():
    st.session_state['lang_setting'] = st.session_state.get("language_select")

def paging():
    st.page_link("streamlit_app.py", label="Home", icon="ğŸ ")
    st.page_link("pages/two_agents.py", label="Two Agents' Talk", icon="ğŸ’­")

def append_wordcloud_to_history(df, vectorizer, tfidf_matrix, query):
    fig = udn_data_processing.draw_wordcloud(df, vectorizer, tfidf_matrix, query=query)
    buf = io.BytesIO()
    fig.savefig(buf, format="png")
    buf.seek(0)
    img_bytes = buf.read()
    st.session_state.messages.append({
    "role": "assistant",
    "type": "wordcloud",
    "image_bytes": img_bytes,
    "text_before": f"å¥½çš„ï¼Œæ­£åœ¨ç‚ºä½ ç”Ÿæˆã€Œ{query}ã€çš„è©é›²â€¦",
    "text_after": "é€™æ˜¯ä½ çš„è©é›²ï¼Œå¸Œæœ›å°ä½ æœ‰å¹«åŠ©ï¼"
})

def main():
    udn_data_processing.ensure_font()

    if "wc_stage" not in st.session_state:
        st.session_state.wc_stage = 0
    if "wc_query" not in st.session_state:
        st.session_state.wc_query = ""
    if "recommend_triggered" not in st.session_state:
        st.session_state.recommend_triggered = False
    if "recommend_query" not in st.session_state:
        st.session_state.recommend_query = ""

    st.set_page_config(
        page_title='K-Assistant - The Residemy Agent',
        layout='wide',
        initial_sidebar_state='auto',
        menu_items={
            'Get Help': 'https://streamlit.io/',
            'Report a bug': 'https://github.com',
            'About': 'About your application: **Hello world**'
            },
        page_icon="img/favicon.ico"
    )

    df, vectorizer, tfidf_matrix = load_data()
    # Show title and description.
    st.title(f"ğŸ’¬ {user_name}'s Chatbot")

    with st.sidebar:
        paging()
        selected_lang = st.selectbox("Language", ["English", "ç¹é«”ä¸­æ–‡"], index=0, on_change=save_lang, key="language_select")
        if 'lang_setting' in st.session_state:
            lang_setting = st.session_state['lang_setting']
        else:
            lang_setting = selected_lang
            st.session_state['lang_setting'] = lang_setting
        

        st_c_1 = st.container(border=True)
        with st_c_1:
            st.image("https://www.w3schools.com/howto/img_avatar.png")

    st_c_chat = st.container(border=True)
    with st_c_chat:
        if "messages" not in st.session_state:
            st.session_state.messages = []
        else:
            for msg in st.session_state.messages:
                if msg.get("type") == "wordcloud":
                    st.chat_message("assistant", avatar=user_image).write(msg["text_before"])
                    st.chat_message("assistant", avatar=user_image).image(Image.open(io.BytesIO(msg["image_bytes"])))
                    st.chat_message("assistant", avatar=user_image).write(msg["text_after"])
                elif msg["role"] == "user":
                    if user_image:
                        st_c_chat.chat_message(msg["role"],avatar=user_image).markdown((msg["content"]))
                    else:
                        st_c_chat.chat_message(msg["role"]).markdown((msg["content"]))
                elif msg["role"] == "assistant":
                    st_c_chat.chat_message(msg["role"]).markdown((msg["content"]))
                else:
                    try:
                        image_tmp = msg.get("image")
                        if image_tmp:
                            st_c_chat.chat_message(msg["role"],avatar=image_tmp).markdown((msg["content"]))
                    except:
                        st_c_chat.chat_message(msg["role"]).markdown((msg["content"]))


        story_template = ("Please give appropriate response according to the content of ##PROMPT##."
                        f"And remeber to mention user's name {user_name} in the end. Don't add some emoji in the end of each sentence."
                        f"Please express in {lang_setting}")

        classification_template = ("You are a classification agent, your job is to classify what ##PROMPT## is according to the job definition list in <JOB_DEFINITION>"
        "<JOB_DEFINITION>"
        f"{JOB_DEFINITION}"
        "</JOB_DEFINITION>"
        "Please output in JSON-format only."
        "JSON-format is as below:"
        f"{RESPONSE_FORMAT}"
        "Let's think step by step."
        f"Please output in {lang_setting}"
        )

        def generate_response(prompt):

            # prompt_template = f"Give me a story started from '{prompt}'"
            prompt_template = story_template.replace('##PROMPT##',prompt)
            # prompt_template = classification_template.replace('##PROMPT##',prompt)
            result = user_proxy.initiate_chat(
            recipient=assistant,
            message=prompt_template
            )

            response = result.summary
            return response

        def show_chat_history(chat_hsitory):
            for entry in chat_hsitory:
                role = entry.get('role')
                name = entry.get('name')
                content = entry.get('content')
                st.session_state.messages.append({"role": f"{role}", "content": content})

                if len(content.strip()) != 0: 
                    if 'ALL DONE' in content:
                        return 
                    else: 
                        if role != 'assistant':
                            st_c_chat.chat_message(f"{role}").write((content))
                        else:
                            st_c_chat.chat_message("user",avatar=user_image).write(content)
        
            return 
        
        def wants_recommendation(prompt: str) -> bool:
            sys_instruction = """
        You are an intent classifier. Decide whether the user is asking for game recommendations.
        As long as there are words related to the game and recommend(éŠæˆ²ã€æ¨è–¦), it can be regarded as the user wants to be recommended
        Respond in JSON ONLY, with a boolean field "recommend".
        Some Examples:
        Input: "æˆ‘æƒ³æ‰¾ä¸€äº›å¥½ç©çš„éŠæˆ²"
        Output: {"recommend": true}
        Input: "ä»Šå¤©å¤©æ°£æ€éº¼æ¨£ï¼Ÿ"
        Output: {"recommend": false}
        Input: "å¹«æˆ‘æ¨è–¦ RPG é¡å‹çš„éŠæˆ²"
        Output: {"recommend": true}
        Input: "ä½ å–œæ­¡ä»€éº¼é¡è‰²ï¼Ÿ"
        Output: {"recommend": false}
        Now classify the final input.  (Do NOT output anything else.)
        """
            gemini_client = genai.Client(api_key=GEMINI_API_KEY)
            resp = gemini_client.models.generate_content(
                model="gemini-2.0-flash-lite",
                contents=prompt,
                config=types.GenerateContentConfig(
                    system_instruction=sys_instruction,
                    max_output_tokens=50
                )
            )

            text = resp.text.strip()
            match = re.search(r'\{.*\}', text)
            if match:
                try:
                    j = json.loads(match.group())
                    return bool(j.get("recommend", False))
                except json.JSONDecodeError:
                    pass
            return False
        
        def chat(prompt: str):
            st_c_chat.chat_message("user",avatar=user_image).write(prompt)
            st.session_state.messages.append({"role": "user", "content": prompt})
            if wants_recommendation(prompt):
                recs = recommend_games(prompt, df, vectorizer, tfidf_matrix, top_n=5)
                st.markdown("### ğŸ¯ æˆ‘çŒœä½ å¯èƒ½æœ‰èˆˆè¶£çš„æ–‡ç« ï¼éŠæˆ²")
                st.table(
                    recs[["title","url","score"]]
                    .assign(score=lambda df: df["score"].map(lambda x: f"{x:.3f}"))
                )
                context = ""
                for _, row in recs.iterrows():
                    snippet = row["content"][:300].replace("\n"," ")
                    context += f"æ–‡ç« æ¨™é¡Œï¼š{row['title']}\næ‘˜è¦ï¼š{snippet} â€¦\n\n"

                # 4. è®“ LLM æ ¹æ“šé€™äº›å…§å®¹åšç°¡çŸ­ä»‹ç´¹
                summary_prompt = (
                    "ä»¥ä¸‹æ˜¯å…©ç¯‡éŠæˆ²å¿ƒå¾—æ–‡ç« çš„æ¨™é¡Œèˆ‡å…§å®¹æ‘˜è¦ï¼Œ"
                    "è«‹åˆ†åˆ¥ç”¨ 2â€‘3 å¥è©±ï¼Œä»‹ç´¹é€™å…©æ¬¾éŠæˆ²çš„ä¸»è¦ç‰¹è‰²èˆ‡ç©æ³•ï¼š\n\n"
                    f"{context}"
                )
                intro = generate_response(summary_prompt)

                st.markdown("### ğŸ“– æ¨è–¦éŠæˆ²ç°¡ä»‹")
                st.write(intro)
            else:
                reply = generate_response(prompt)
                st_c_chat.chat_message("assistant").write(reply)
                st.session_state.messages.append({"role": "assistant", "content": reply})
            
    if prompt := st.chat_input(placeholder=placeholderstr, key="chat_bot"):
        chat(prompt)

    def to_stage1():
        st.session_state.wc_stage = 1

    def choose_all():
        append_wordcloud_to_history(df, vectorizer, tfidf_matrix, query="all")
        st.session_state.wc_stage = 0

    def to_stage2():
        st.session_state.wc_stage = 2

    def submit_query():
        q = st.session_state.wc_query.strip()
        if q:
            append_wordcloud_to_history(df, vectorizer, tfidf_matrix, query=q)
        st.session_state.wc_query = ""
        st.session_state.wc_stage = 0

    def trigger_recommend():
            st.session_state.recommend_triggered = True
    
    def reset_recommend():
        st.session_state.recommend_triggered = False
        st.session_state.recommend_query = ""

    def run_recommend_flow(user_input):
            """
            é€™æ®µè² è²¬å‘¼å«æ—¢æœ‰çš„ recommend_games() + generate_response()ï¼Œé¡¯ç¤ºçµæœã€‚
            """
            st.session_state.recommend_triggered = False
            st.session_state.pop("recommend_query", None)
            recs = recommend_games(user_input, df, vectorizer, tfidf_matrix, top_n=5)
            df_for_md = recs[["title", "url", "score"]].copy()
            df_for_md["score"] = df_for_md["score"].map(lambda x: f"{x:.3f}")
            df_for_md["title"] = df_for_md.apply(lambda row: f"[{row['title']}]({row['url']})", axis=1)
            md_table = df_for_md.to_markdown(index=False)
            context = ""
            for _, row in recs.iterrows():
                snippet = row["content"][:300].replace("\n", " ")
                context += f"æ–‡ç« æ¨™é¡Œï¼š{row['title']}\næ‘˜è¦ï¼š{snippet} â€¦\n\n"
            summary_prompt = (
                "ä»¥ä¸‹æ˜¯å…©ç¯‡éŠæˆ²å¿ƒå¾—æ–‡ç« çš„æ¨™é¡Œèˆ‡å…§å®¹æ‘˜è¦ï¼Œ"
                "è«‹åˆ†åˆ¥ç”¨ 2-3 å¥è©±ï¼Œä»‹ç´¹é€™å…©æ¬¾éŠæˆ²çš„ä¸»è¦ç‰¹è‰²èˆ‡ç©æ³•ï¼š\n\n"
                f"{context}"
            )
            intro = generate_response(summary_prompt)
            markdown_str = (
                "### ğŸ¯ æˆ‘çŒœä½ å¯èƒ½æœ‰èˆˆè¶£çš„æ–‡ç« ï¼éŠæˆ²\n\n"
                f"{md_table}\n\n" 
                "### ğŸ“– æ¨è–¦éŠæˆ²ç°¡ä»‹\n\n"
                f"{intro}"
            )
            st.session_state.messages.append({
                "role": "assistant",
                "content": markdown_str
            })

    def on_submit_recommend():
        user_qry = st.session_state.get("recommend_query", "").strip()
        if user_qry:
            run_recommend_flow(user_qry)
        else:
            st.warning("è«‹å…ˆè¼¸å…¥æƒ³è¦æœå°‹çš„é—œéµå­—ï¼æè¿°")


    ctrl_container = st.container()
    with ctrl_container:
        if not st.session_state.recommend_triggered:
            st.button("ğŸ”è«‹æ¨è–¦éŠæˆ²çµ¦æˆ‘", on_click=trigger_recommend)
        else:
                st.markdown(
                    "æ²’å•é¡Œï¼è«‹è¼¸å…¥ä½ æƒ³çŸ¥é“çš„ä¸»é¡Œå…ƒç´ ï¼Œå¯ä»¥æ˜¯å­—è©ä¹Ÿå¯ä»¥æ˜¯ä¸€æ®µæè¿°ï¼Œ"
                    "å¦‚æœæè¿°å¾—è¶Šæ¸…æ¥šï¼Œæˆ‘è¶Šèƒ½æ›´æº–ç¢ºåœ°æ¨è–¦ä½ æƒ³è¦çš„éŠæˆ²å“¦ï¼"
                )
                user_qry = st.text_input("", key="recommend_query", placeholder="è«‹åœ¨æ­¤è¼¸å…¥æ¨è–¦é—œéµå­—")
                st.button("æäº¤", on_click=on_submit_recommend)
            

        if st.session_state.wc_stage == 0:
            st.button("ğŸ“‹ æˆ‘æƒ³æŸ¥çœ‹è©é›²", on_click=to_stage1)

        elif st.session_state.wc_stage == 1:
            st.write("ä½ å¥½ï¼Œè«‹å•ä½ æƒ³è¦çœ‹æ•´å€‹è³‡æ–™åº«çš„è©é›²ï¼Œé‚„æ˜¯ä½ æƒ³è¦å°‹æ‰¾ç‰¹å®šä¸»é¡Œçš„è©é›²ï¼Ÿ")
            st.button("ğŸ“¦ æˆ‘æƒ³è¦å…¨éƒ¨", on_click=choose_all)
            st.button("ğŸ” æˆ‘æƒ³çŸ¥é“ç‰¹å®šä¸»é¡Œçš„è©é›²", on_click=to_stage2)
        
        if st.session_state.wc_stage == 2:
            st.markdown("æ²’å•é¡Œï¼è«‹è¼¸å…¥ä½ æƒ³çŸ¥é“çš„ä¸»é¡Œå…ƒç´ ï¼Œå¯ä»¥æ˜¯å­—è©ä¹Ÿå¯ä»¥æ˜¯ä¸€æ®µæè¿°ï¼Œå¦‚æœæè¿°å¾—è¶Šæ¸…æ¥šï¼Œæˆ‘è¶Šèƒ½æ›´æº–ç¢ºçš„æ‰¾åˆ°ä½ æƒ³è¦çš„è©é›²å“¦ï¼")
            query = st.text_input(
            "", key="wc_query")
            if st.button("æäº¤", on_click=submit_query):
                pass
       
        with st.expander("ğŸ“Š é¡¯ç¤ºåŸå§‹è³‡æ–™èˆ‡ LLM è™•ç†çµæœ"):
            st.subheader("Preview of Raw Data")
            st.dataframe(df)

            raw_col = 'tokenize and stop words without remove control'
            if raw_col in df.columns:
                st.subheader("Refining tokens with LLM skill")
                df['tokens_refined'] = df[raw_col].apply(refine_tokens_list)
                st.dataframe(df[[raw_col, 'tokens_refined']])
            else:
                st.warning(f"Column '{raw_col}' not found in CSV.")

if __name__ == "__main__":
    main()
